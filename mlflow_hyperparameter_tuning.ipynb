{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703daca1",
   "metadata": {},
   "source": [
    "# MLflow Hyperparameter Tuning Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use MLflow for experiment tracking and hyperparameter optimization with the MNIST dataset.\n",
    "\n",
    "**MNIST Dataset**: A collection of 28Ã—28 pixel grayscale images of handwritten digits (0-9), containing 70,000 samples total.\n",
    "\n",
    "**Prerequisites**:\n",
    "- MLflow server running (default: `http://127.0.0.1:5000`)\n",
    "- Required libraries: `mlflow`, `hyperopt`, `scikit-learn`, `numpy`, `pandas`\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Configure MLflow tracking\n",
    "# os.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5000'\n",
    "# mlflow.set_tracking_uri('http://127.0.0.1:5000/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fc2db",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial covers:\n",
    "\n",
    "1. **Data Management**: Proper train/validation/test splits with reproducibility\n",
    "2. **Baseline Model**: Training and logging a baseline logistic regression model\n",
    "3. **Hyperparameter Tuning**: Using Hyperopt for automated hyperparameter optimization\n",
    "4. **Best Practices**: \n",
    "   - Reproducible experiments with fixed random seeds\n",
    "   - Proper data splits to avoid overfitting\n",
    "   - Comprehensive MLflow logging (parameters, metrics, models, examples)\n",
    "   - Test set evaluation for final model comparison\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Reproducibility**: Using fixed random seeds ensures anyone running the code gets identical results\n",
    "- **Data Splitting Tradeoffs**: Larger validation/test sets provide more reliable estimates but reduce training data\n",
    "- **Hyperparameter Optimization**: Systematic search for optimal model configurations\n",
    "- **MLflow Tracking**: Comprehensive logging of experiments for comparison and reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # To keep the output clean\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9408a32d",
   "metadata": {},
   "source": [
    "## Step 1: Load and Preprocess Data\n",
    "\n",
    "In this step, we:\n",
    "- Load the MNIST dataset from OpenML\n",
    "- Normalize the features using StandardScaler\n",
    "- Split the data into train/validation/test sets with proper stratification\n",
    "- Ensure reproducibility using fixed random seeds\n",
    "\n",
    "**Key Points:**\n",
    "- Using `random_state` ensures anyone running this code gets identical data splits\n",
    "- Stratified splitting maintains class distribution across splits\n",
    "- Test set is reserved for final evaluation only (not used during training or tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess MNIST dataset with proper train/validation/test splits.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, scaler, split_info\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist['data'], mnist['target'].astype(np.int64)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Sample\n",
    "    n_samples = 1_000\n",
    "    indices = np.random.choice(len(X), size=n_samples, replace=False)\n",
    "    X_sample = X[indices]\n",
    "    y_sample = y[indices]\n",
    "\n",
    "\n",
    "    # First split: separate test set (10k samples)\n",
    "    # Using random_state ensures reproducibility - anyone running this code gets the same splits\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_sample, y_sample, test_size=100, random_state=RANDOM_STATE, stratify=y_sample\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation sets (60k train, 10k val)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=100, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Data splits - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Log split sizes for reproducibility\n",
    "    split_info = {\n",
    "        'train_size': len(X_train),\n",
    "        'val_size': len(X_val),\n",
    "        'test_size': len(X_test),\n",
    "        'random_state': RANDOM_STATE\n",
    "    }\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, split_info\n",
    "\n",
    "# Load the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler, split_info = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a94358",
   "metadata": {},
   "source": [
    "## Step 2: Train and Log Baseline Model\n",
    "\n",
    "We establish a baseline model to:\n",
    "- Set a performance benchmark for comparison\n",
    "- Demonstrate proper MLflow logging practices\n",
    "- Log all parameters, metrics, and model artifacts\n",
    "\n",
    "**What we log:**\n",
    "- Model hyperparameters (C, max_iter, solver, etc.)\n",
    "- Data split information for reproducibility\n",
    "- Validation and test set accuracies\n",
    "- Model signature and input examples\n",
    "- Tags for easy filtering in MLflow UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42bb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_baseline(X_train, X_val, y_train, y_val, X_test, y_test, split_info):\n",
    "    \"\"\"\n",
    "    Train a baseline logistic regression model and log it to MLflow.\n",
    "    \n",
    "    Note: We use the same parameter values for both model instantiation and logging\n",
    "    to ensure consistency and avoid discrepancies.\n",
    "    \"\"\"\n",
    "    experiment_name = \"MNIST_LogisticRegression\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"baseline_logreg\") as run:\n",
    "        # Model: multinomial logistic regression with lbfgs solver (best for multinomial)\n",
    "        C = 1.0\n",
    "        max_iter = 100\n",
    "        \n",
    "        # Use the same parameters for both model and logging to ensure consistency\n",
    "        model = LogisticRegression(\n",
    "            multi_class='multinomial', \n",
    "            solver='lbfgs', \n",
    "            max_iter=max_iter, \n",
    "            C=C, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        print(\"Training baseline logistic regression model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        acc_val = accuracy_score(y_val, y_pred_val)\n",
    "        print(f\"Baseline Validation Accuracy: {acc_val:.4f}\")\n",
    "        \n",
    "        # Evaluate on test set (for final comparison)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        acc_test = accuracy_score(y_test, y_pred_test)\n",
    "        print(f\"Baseline Test Accuracy: {acc_test:.4f}\")\n",
    "        \n",
    "        # Log parameters (ensuring they match the model's actual parameters)\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"solver\", \"lbfgs\")\n",
    "        mlflow.log_param(\"multi_class\", \"multinomial\")\n",
    "        mlflow.log_param(\"max_iter\", max_iter)\n",
    "        mlflow.log_param(\"C\", C)\n",
    "        mlflow.log_param(\"random_state\", RANDOM_STATE)\n",
    "        \n",
    "        # Log data split information\n",
    "        for key, value in split_info.items():\n",
    "            mlflow.log_param(f\"data_{key}\", value)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy_val\", acc_val)\n",
    "        mlflow.log_metric(\"accuracy_test\", acc_test)\n",
    "        \n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"dataset\", \"MNIST\")\n",
    "        mlflow.set_tag(\"task\", \"multiclass_classification\")\n",
    "        mlflow.set_tag(\"model_version\", \"baseline\")\n",
    "        \n",
    "        # Log input examples (prevents warnings and documents expected input format)\n",
    "        # Sample a few examples from each class\n",
    "        example_indices = []\n",
    "        for digit in range(10):\n",
    "            indices = np.where(y_train == digit)[0]\n",
    "            if len(indices) > 0:\n",
    "                example_indices.append(indices[0])\n",
    "        \n",
    "        examples = X_train[example_indices[:5]]  # Log 5 examples\n",
    "        \n",
    "        # Create model signature with input examples\n",
    "        signature = infer_signature(examples, model.predict(examples))\n",
    "        \n",
    "        # Log model with signature and input examples\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\", signature=signature, input_example=examples)\n",
    "        \n",
    "        return run.info.run_id, model\n",
    "\n",
    "# Train and log baseline model\n",
    "print(\"=\"*60)\n",
    "print(\"Training Baseline Model\")\n",
    "print(\"=\"*60)\n",
    "baseline_run_id, baseline_model = train_and_log_baseline(\n",
    "    X_train, X_val, y_train, y_val, X_test, y_test, split_info\n",
    ")\n",
    "print(f\"Baseline model run ID: {baseline_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6889f0",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning with Hyperopt\n",
    "\n",
    "We use Hyperopt's Tree-structured Parzen Estimator (TPE) algorithm to efficiently search the hyperparameter space.\n",
    "\n",
    "**Hyperparameters being tuned:**\n",
    "- **C**: Inverse of regularization strength (log-uniform distribution: 1e-4 to 10)\n",
    "- **max_iter**: Maximum iterations for solver convergence (quantized uniform: 50-300)\n",
    "\n",
    "**Key Concepts:**\n",
    "- TPE is a Bayesian optimization method that learns from previous trials\n",
    "- We optimize on the validation set (test set remains untouched)\n",
    "- **All trials are logged to MLflow as nested runs** in a parent run group for live comparison\n",
    "- You can watch the hyperparameter search progress in real-time in the MLflow UI\n",
    "- Each trial is logged with its hyperparameters and validation accuracy for easy comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1da5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperopt_train_eval(params, X_train, X_val, y_train, y_val, trial_num, parent_run_id, split_info):\n",
    "    \"\"\"\n",
    "    Objective function for hyperopt with MLflow logging for each trial.\n",
    "    \n",
    "    Hyperparameters:\n",
    "    - C: Inverse of regularization strength (smaller = stronger regularization)\n",
    "    - max_iter: Maximum number of iterations for solver convergence\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with loss (negative accuracy), status, and model\n",
    "    \"\"\"\n",
    "    C = params['C']\n",
    "    max_iter = int(params['max_iter'])\n",
    "    \n",
    "    # Log each trial as a child run in the parent run group\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial_num}\", nested=True) as child_run:\n",
    "        model = LogisticRegression(\n",
    "            multi_class='multinomial', \n",
    "            solver='lbfgs', \n",
    "            max_iter=max_iter, \n",
    "            C=C, \n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        # Log hyperparameters\n",
    "        mlflow.log_param(\"C\", float(C))\n",
    "        mlflow.log_param(\"max_iter\", int(max_iter))\n",
    "        mlflow.log_param(\"trial_num\", trial_num)\n",
    "        \n",
    "        # Log data split information\n",
    "        for key, value in split_info.items():\n",
    "            mlflow.log_param(f\"data_{key}\", value)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy_val\", acc)\n",
    "        mlflow.log_metric(\"loss\", -acc)  # Loss for hyperopt (negative accuracy)\n",
    "        \n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"dataset\", \"MNIST\")\n",
    "        mlflow.set_tag(\"task\", \"multiclass_classification\")\n",
    "        mlflow.set_tag(\"tuning\", \"hyperopt\")\n",
    "        mlflow.set_tag(\"trial_type\", \"hyperparameter_search\")\n",
    "        \n",
    "        # Log model (optional - can be commented out to save space)\n",
    "        # mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "    \n",
    "    # We want to maximize accuracy, so return negative loss\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model, 'accuracy': acc}\n",
    "\n",
    "def tune_hyperparameters(X_train, X_val, y_train, y_val, split_info, max_evals=30):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters using Hyperopt with Tree-structured Parzen Estimator (TPE).\n",
    "    All trials are logged to MLflow in a grouped run for live comparison.\n",
    "    \n",
    "    Search space:\n",
    "    - C: Log-uniform distribution from 1e-4 to 10 (common range for regularization)\n",
    "    - max_iter: Uniform quantized distribution from 50 to 300 (step size 10)\n",
    "    \n",
    "    All trial results are logged to MLflow as child runs for real-time comparison.\n",
    "    \"\"\"\n",
    "    experiment_name = \"MNIST_LogisticRegression\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    # Create a parent run to group all hyperparameter tuning trials\n",
    "    with mlflow.start_run(run_name=\"hyperopt_tuning_group\") as parent_run:\n",
    "        parent_run_id = parent_run.info.run_id\n",
    "        \n",
    "        # Log parent run metadata\n",
    "        mlflow.set_tag(\"dataset\", \"MNIST\")\n",
    "        mlflow.set_tag(\"task\", \"multiclass_classification\")\n",
    "        mlflow.set_tag(\"tuning\", \"hyperopt\")\n",
    "        mlflow.set_tag(\"run_type\", \"hyperparameter_tuning_group\")\n",
    "        mlflow.log_param(\"max_evals\", max_evals)\n",
    "        mlflow.log_param(\"optimization_algorithm\", \"TPE\")\n",
    "        \n",
    "        # Define search space\n",
    "        # Using loguniform for C since regularization strength varies over orders of magnitude\n",
    "        space = {\n",
    "            'C': hp.loguniform('C', np.log(1e-4), np.log(10)),\n",
    "            'max_iter': hp.quniform('max_iter', 50, 300, 10)\n",
    "        }\n",
    "        \n",
    "        # Note: Optimizing on validation set can lead to overfitting to validation data.\n",
    "        # In production, use cross-validation or a separate validation set for final tuning.\n",
    "        \n",
    "        # Track trial number for run naming\n",
    "        trial_counter = [0]  # Use list to allow modification in nested function\n",
    "        \n",
    "        def objective_with_logging(params):\n",
    "            trial_counter[0] += 1\n",
    "            return hyperopt_train_eval(\n",
    "                params, X_train, X_val, y_train, y_val, \n",
    "                trial_counter[0], parent_run_id, split_info\n",
    "            )\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(\n",
    "            fn=objective_with_logging,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,  # Tree-structured Parzen Estimator algorithm\n",
    "            max_evals=max_evals,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(RANDOM_STATE)  # Reproducibility\n",
    "        )\n",
    "        \n",
    "        # Extract best model from trials\n",
    "        best_trial = trials.best_trial\n",
    "        best_acc = -best_trial['result']['loss']\n",
    "        best_model = best_trial['result']['model']\n",
    "        \n",
    "        # Log summary metrics to parent run\n",
    "        mlflow.log_metric(\"best_accuracy_val\", best_acc)\n",
    "        mlflow.log_metric(\"n_trials_completed\", len(trials.trials))\n",
    "        mlflow.log_param(\"best_C\", float(best['C']))\n",
    "        mlflow.log_param(\"best_max_iter\", int(best['max_iter']))\n",
    "        \n",
    "        # Log all trial results as a summary\n",
    "        trial_results = {\n",
    "            'n_trials': len(trials.trials),\n",
    "            'best_accuracy': float(best_acc),\n",
    "            'best_params': {k: float(v) if isinstance(v, (int, float, np.number)) else v \n",
    "                           for k, v in best.items()},\n",
    "            'parent_run_id': parent_run_id\n",
    "        }\n",
    "    \n",
    "    return best, best_acc, best_model, trial_results\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Hyperparameter Tuning with Hyperopt\")\n",
    "print(\"=\"*60)\n",
    "print(\"All trials will be logged to MLflow for live comparison...\")\n",
    "best_params, best_acc, best_model, trial_results = tune_hyperparameters(\n",
    "    X_train, X_val, y_train, y_val, split_info, max_evals=30\n",
    ")\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best validation accuracy: {best_acc:.4f}\")\n",
    "print(f\"Total trials: {trial_results['n_trials']}\")\n",
    "print(f\"Parent run ID: {trial_results['parent_run_id']}\")\n",
    "print(\"View all trials grouped together in MLflow UI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b466256",
   "metadata": {},
   "source": [
    "## Step 4: Log Best Tuned Model\n",
    "\n",
    "After hyperparameter tuning, we log the best model to MLflow with:\n",
    "- All hyperparameters that were found\n",
    "- Validation and test set performance\n",
    "- Model signature and input examples\n",
    "- Tags indicating this is a tuned model\n",
    "\n",
    "This allows easy comparison with the baseline model in the MLflow UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_best_model(best_params, best_acc, best_model, X_train, X_val, X_test, \n",
    "                   y_train, y_val, y_test, split_info, trial_results):\n",
    "    \"\"\"\n",
    "    Log the best model from hyperparameter tuning to MLflow.\n",
    "    \"\"\"\n",
    "    experiment_name = \"MNIST_LogisticRegression\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"tuned_logreg\") as run:\n",
    "        # Evaluate on test set\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        acc_test = accuracy_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"solver\", \"lbfgs\")\n",
    "        mlflow.log_param(\"multi_class\", \"multinomial\")\n",
    "        mlflow.log_param(\"C\", float(best_params['C']))\n",
    "        mlflow.log_param(\"max_iter\", int(best_params['max_iter']))\n",
    "        mlflow.log_param(\"random_state\", RANDOM_STATE)\n",
    "        \n",
    "        # Log data split information\n",
    "        for key, value in split_info.items():\n",
    "            mlflow.log_param(f\"data_{key}\", value)\n",
    "        \n",
    "        # Log hyperopt trial summary\n",
    "        mlflow.log_param(\"n_trials\", trial_results['n_trials'])\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy_val\", best_acc)\n",
    "        mlflow.log_metric(\"accuracy_test\", acc_test)\n",
    "        \n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"dataset\", \"MNIST\")\n",
    "        mlflow.set_tag(\"task\", \"multiclass_classification\")\n",
    "        mlflow.set_tag(\"tuning\", \"hyperopt\")\n",
    "        mlflow.set_tag(\"model_version\", \"tuned\")\n",
    "        \n",
    "        # Log input examples\n",
    "        example_indices = []\n",
    "        for digit in range(10):\n",
    "            indices = np.where(y_train == digit)[0]\n",
    "            if len(indices) > 0:\n",
    "                example_indices.append(indices[0])\n",
    "        \n",
    "        examples = X_train[example_indices[:5]]\n",
    "        \n",
    "        # Create model signature with input examples\n",
    "        signature = infer_signature(examples, best_model.predict(examples))\n",
    "        \n",
    "        # Log model with signature and input examples\n",
    "        mlflow.sklearn.log_model(best_model, artifact_path=\"model\", signature=signature, input_example=examples)\n",
    "        \n",
    "        print(f\"Tuned model logged - Val Accuracy: {best_acc:.4f}, Test Accuracy: {acc_test:.4f}\")\n",
    "        return run.info.run_id, best_model\n",
    "\n",
    "# Log the best tuned model\n",
    "print(\"=\"*60)\n",
    "print(\"Logging Best Tuned Model\")\n",
    "print(\"=\"*60)\n",
    "tuned_run_id, tuned_model = log_best_model(\n",
    "    best_params, best_acc, best_model, X_train, X_val, X_test,\n",
    "    y_train, y_val, y_test, split_info, trial_results\n",
    ")\n",
    "print(f\"Tuned model run ID: {tuned_run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710b941",
   "metadata": {},
   "source": [
    "## Step 5: Compare Models on Test Set\n",
    "\n",
    "Finally, we compare both models on the held-out test set to get an unbiased estimate of their performance. This is the only time we use the test set - it was not used during training or hyperparameter tuning.\n",
    "\n",
    "**Why this matters:**\n",
    "- The test set provides an unbiased estimate of model performance\n",
    "- Comparing models on the same test set ensures fair comparison\n",
    "- The classification report shows per-class performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b269d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(baseline_model, tuned_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compare baseline and tuned models on the test set.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Final Model Comparison on Test Set\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    baseline_pred = baseline_model.predict(X_test)\n",
    "    tuned_pred = tuned_model.predict(X_test)\n",
    "    \n",
    "    baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "    tuned_acc = accuracy_score(y_test, tuned_pred)\n",
    "    \n",
    "    print(f\"\\nBaseline Model Test Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"Tuned Model Test Accuracy: {tuned_acc:.4f}\")\n",
    "    print(f\"Improvement: {tuned_acc - baseline_acc:.4f} ({((tuned_acc - baseline_acc) / baseline_acc * 100):.2f}%)\")\n",
    "    \n",
    "    print(\"\\nTuned Model Classification Report:\")\n",
    "    print(classification_report(y_test, tuned_pred))\n",
    "    \n",
    "    return baseline_acc, tuned_acc\n",
    "\n",
    "# Compare models on test set\n",
    "baseline_test_acc, tuned_test_acc = compare_models(baseline_model, tuned_model, X_test, y_test)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"View your experiments at: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882dbcec",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully completed a comprehensive MLflow hyperparameter tuning workflow. Here's what we accomplished:\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Reproducibility**: \n",
    "   - Fixed random seeds ensure identical results across runs\n",
    "   - Data split sizes are logged for transparency\n",
    "   - All hyperparameters are tracked in MLflow\n",
    "\n",
    "2. **Proper Data Management**:\n",
    "   - Separate train/validation/test splits prevent data leakage\n",
    "   - Test set is only used for final evaluation, not during tuning\n",
    "   - Stratified splits maintain class distribution\n",
    "\n",
    "3. **Comprehensive Logging**:\n",
    "   - All model parameters are logged and match actual model configuration\n",
    "   - Input examples are logged to document expected data format\n",
    "   - Both validation and test metrics are tracked\n",
    "   - Hyperparameter search space and results are documented\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Baseline model establishes a performance benchmark\n",
    "   - Hyperparameter tuning uses validation set (test set reserved for final evaluation)\n",
    "   - Model comparison on held-out test set provides unbiased performance estimates\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore MLflow UI to compare runs and visualize metrics\n",
    "- Experiment with different hyperparameter search spaces\n",
    "- Try other optimization algorithms (e.g., random search, Bayesian optimization)\n",
    "- Consider cross-validation for more robust hyperparameter selection\n",
    "- Extend to other model types (neural networks, ensemble methods)\n",
    "\n",
    "This workflow ensures scientific rigor and reproducibility in your machine learning experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c83b4-9d1c-448e-b63e-33553478ebb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
